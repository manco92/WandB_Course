{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import json\n",
    "from datasets import Dataset, DatasetDict\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(r\"C:\\Users\\Leandro\\Desktop\\WandB_Projects\\LLM_alpaca\\alpaca_data_cleaned.json\")\n",
    "data = json.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establecer la semilla para reproducibilidad\n",
    "np.random.seed(42)\n",
    "\n",
    "# Definir el rango de números (0 a 49999)\n",
    "rango_numeros = np.arange(len(data))\n",
    "\n",
    "# Generar lista_valid con 1000 números aleatorios sin repetición\n",
    "lista_valid = np.random.choice(rango_numeros, size=1000, replace=False)\n",
    "\n",
    "# Generar lista_train con los restantes 49000 números\n",
    "lista_train = np.setdiff1d(rango_numeros, lista_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establecer la semilla para reproducibilidad\n",
    "np.random.seed(42)\n",
    "\n",
    "# Definir el rango de números (0 a 49999)\n",
    "rango_numeros = np.arange(len(data))\n",
    "\n",
    "# Generar lista_valid con 1000 números aleatorios sin repetición\n",
    "lista_valid = np.random.choice(rango_numeros, size=1000, replace=False)\n",
    "\n",
    "# Generar lista_train con los restantes 49000 números\n",
    "lista_train = np.setdiff1d(rango_numeros, lista_valid)\n",
    "\n",
    "data_train = np.array(data)[lista_train]\n",
    "data_valid = np.array(data)[lista_valid]\n",
    "\n",
    "# Convertir la lista de diccionarios a un diccionario de listas\n",
    "data_dict_train = {key: [item[key] for item in data_train] for key in data_train[0]}\n",
    "data_dict_valid = {key: [item[key] for item in data_valid] for key in data_valid[0]}\n",
    "\n",
    "# Crear un objeto Dataset a partir de la lista de diccionarios\n",
    "custom_dataset_train = Dataset.from_dict(data_dict_train)\n",
    "custom_dataset_valid = Dataset.from_dict(data_dict_valid)\n",
    "\n",
    "# Convertir el Dataset en un DatasetDict\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': custom_dataset_train,\n",
    "    'valid': custom_dataset_valid})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset_dict['train']\n",
    "eval_dataset = dataset_dict['valid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_no_input(row):\n",
    "    return (\"Below is an instruction that describes a task. \"\n",
    "            \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "            \"### Instruction:\\n{instruction}\\n\\n### Response:\\n{output}\").format_map(row)\n",
    "\n",
    "def prompt_input(row):\n",
    "    return (\"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "            \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "            \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n{output}\").format_map(row)\n",
    "\n",
    "def create_prompt(row):\n",
    "    return prompt_no_input(row) if row[\"input\"] == \"\" else prompt_input(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_id = 'openlm-research/open_llama_3b_v2'\n",
    "#model_id = 'tiiuae/falcon-7b'\n",
    "#model_id = 'bigscience/bloom-3b'\n",
    "model_id = 'tiiuae/falcon-rw-1b'\n",
    "#model_id = r'C:\\Users\\Leandro\\.cache\\huggingface\\hub\\models--bigscience--bloom-3b\\snapshots\\52bc5b43010b4844513826b8be3f78c7344c37d7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=64,  # the rank of the LoRA matrices\n",
    "    lora_alpha=16, # the weight\n",
    "    lora_dropout=0.1, # dropout to add to the LoRA layers\n",
    "    bias=\"none\", # add bias to the nn.Linear layers?\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    #target_modules=[\"q_proj\", \"k_proj\",\"v_proj\",\"o_proj\"], # the name of the layers to add LoRA (LLAMA 3B)\n",
    "    #target_modules=['query_key_value'], # the name of the layers to add LoRA (Falcon 7B)\n",
    "    target_modules=['query_key_value'], # the name of the layers to add LoRA (BLOOM 3B)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kwargs = dict(\n",
    "    device_map=device,\n",
    "    trust_remote_code=True,\n",
    "    # low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    #use_flash_attention_2=True,\n",
    "    use_cache=False,\n",
    "    quantization_config=bnb_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "gradient_accumulation_steps = 16\n",
    "num_train_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./output/falcon-1b\"\n",
    "training_args = TrainingArguments(\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    fp16=True,\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.1,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    #gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs=dict(use_reentrant=False),\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=1,\n",
    "    save_strategy=\"epoch\",\n",
    "    report_to=\"wandb\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Leandro\\source\\repos\\PyTorchTest\\PyTorchTest\\TorchEnv\\lib\\site-packages\\trl\\trainer\\sft_trainer.py:158: UserWarning: You passed a model_id to the SFTTrainer. This will automatically create an `AutoModelForCausalLM` or a `PeftModel` (if you passed a `peft_config`) for you.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin c:\\Users\\Leandro\\source\\repos\\PyTorchTest\\PyTorchTest\\TorchEnv\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda121.dll\n"
     ]
    }
   ],
   "source": [
    "from utils import LLMSampleCB\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model_id,\n",
    "    model_init_kwargs=model_kwargs,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    packing=True,\n",
    "    max_seq_length=1024,\n",
    "    args=training_args,\n",
    "    formatting_func=create_prompt,\n",
    "    peft_config=peft_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from peft import prepare_model_for_kbit_training\n",
    "##trainer.model.config.use_cache = False\n",
    "##trainer.model.gradient_checkpointing_enable()\n",
    "#trainer.model = prepare_model_for_kbit_training(trainer.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32e9968100434be2a4a9dc92b5298fe5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# remove answers\n",
    "def create_prompt_no_anwer(row):\n",
    "    row[\"output\"] = \"\"\n",
    "    return {\"text\": create_prompt(row)}\n",
    "\n",
    "test_dataset = eval_dataset.map(create_prompt_no_anwer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_callback = LLMSampleCB(trainer, test_dataset, num_samples=30, max_new_tokens=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.add_callback(wandb_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mleandro-bello\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Leandro\\Desktop\\WandB_Projects\\LLM_alpaca\\wandb\\run-20240317_142310-n8itsr3a</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/leandro-bello/LLM_alpaca/runs/n8itsr3a' target=\"_blank\">falcon-1b</a></strong> to <a href='https://wandb.ai/leandro-bello/LLM_alpaca' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/leandro-bello/LLM_alpaca' target=\"_blank\">https://wandb.ai/leandro-bello/LLM_alpaca</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/leandro-bello/LLM_alpaca/runs/n8itsr3a' target=\"_blank\">https://wandb.ai/leandro-bello/LLM_alpaca/runs/n8itsr3a</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c627017884640e4a9087d03a08fa744",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6230 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1023, 'learning_rate': 3.210272873194222e-07, 'epoch': 0.0}\n",
      "{'loss': 2.0459, 'learning_rate': 6.420545746388443e-07, 'epoch': 0.0}\n",
      "{'loss': 1.9124, 'learning_rate': 9.630818619582665e-07, 'epoch': 0.0}\n",
      "{'loss': 2.0229, 'learning_rate': 1.2841091492776887e-06, 'epoch': 0.01}\n",
      "{'loss': 1.93, 'learning_rate': 1.6051364365971107e-06, 'epoch': 0.01}\n",
      "{'loss': 2.0723, 'learning_rate': 1.926163723916533e-06, 'epoch': 0.01}\n",
      "{'loss': 2.0631, 'learning_rate': 2.247191011235955e-06, 'epoch': 0.01}\n",
      "{'loss': 2.0747, 'learning_rate': 2.5682182985553774e-06, 'epoch': 0.01}\n",
      "{'loss': 2.0835, 'learning_rate': 2.8892455858747994e-06, 'epoch': 0.01}\n",
      "{'loss': 1.9656, 'learning_rate': 3.2102728731942214e-06, 'epoch': 0.02}\n",
      "{'loss': 1.9379, 'learning_rate': 3.531300160513644e-06, 'epoch': 0.02}\n",
      "{'loss': 1.9743, 'learning_rate': 3.852327447833066e-06, 'epoch': 0.02}\n",
      "{'loss': 1.9185, 'learning_rate': 4.173354735152488e-06, 'epoch': 0.02}\n",
      "{'loss': 1.8956, 'learning_rate': 4.49438202247191e-06, 'epoch': 0.02}\n",
      "{'loss': 1.9691, 'learning_rate': 4.815409309791332e-06, 'epoch': 0.02}\n",
      "{'loss': 1.9978, 'learning_rate': 5.136436597110755e-06, 'epoch': 0.03}\n",
      "{'loss': 2.0093, 'learning_rate': 5.457463884430177e-06, 'epoch': 0.03}\n",
      "{'loss': 1.9232, 'learning_rate': 5.778491171749599e-06, 'epoch': 0.03}\n",
      "{'loss': 2.0389, 'learning_rate': 6.099518459069021e-06, 'epoch': 0.03}\n",
      "{'loss': 1.9067, 'learning_rate': 6.420545746388443e-06, 'epoch': 0.03}\n",
      "{'loss': 2.0009, 'learning_rate': 6.741573033707865e-06, 'epoch': 0.03}\n",
      "{'loss': 2.0009, 'learning_rate': 7.062600321027288e-06, 'epoch': 0.04}\n",
      "{'loss': 1.9954, 'learning_rate': 7.38362760834671e-06, 'epoch': 0.04}\n",
      "{'loss': 1.9975, 'learning_rate': 7.704654895666132e-06, 'epoch': 0.04}\n",
      "{'loss': 1.9627, 'learning_rate': 8.025682182985553e-06, 'epoch': 0.04}\n",
      "{'loss': 1.9971, 'learning_rate': 8.346709470304977e-06, 'epoch': 0.04}\n",
      "{'loss': 1.9821, 'learning_rate': 8.667736757624398e-06, 'epoch': 0.04}\n",
      "{'loss': 2.0295, 'learning_rate': 8.98876404494382e-06, 'epoch': 0.04}\n",
      "{'loss': 2.0089, 'learning_rate': 9.309791332263243e-06, 'epoch': 0.05}\n",
      "{'loss': 1.9953, 'learning_rate': 9.630818619582665e-06, 'epoch': 0.05}\n",
      "{'loss': 1.9356, 'learning_rate': 9.951845906902086e-06, 'epoch': 0.05}\n",
      "{'loss': 1.9747, 'learning_rate': 1.027287319422151e-05, 'epoch': 0.05}\n",
      "{'loss': 1.9275, 'learning_rate': 1.0593900481540931e-05, 'epoch': 0.05}\n",
      "{'loss': 1.9338, 'learning_rate': 1.0914927768860354e-05, 'epoch': 0.05}\n",
      "{'loss': 1.9949, 'learning_rate': 1.1235955056179776e-05, 'epoch': 0.06}\n",
      "{'loss': 2.0421, 'learning_rate': 1.1556982343499198e-05, 'epoch': 0.06}\n",
      "{'loss': 1.9166, 'learning_rate': 1.1878009630818621e-05, 'epoch': 0.06}\n",
      "{'loss': 1.9204, 'learning_rate': 1.2199036918138043e-05, 'epoch': 0.06}\n",
      "{'loss': 1.7918, 'learning_rate': 1.2520064205457466e-05, 'epoch': 0.06}\n",
      "{'loss': 1.9787, 'learning_rate': 1.2841091492776886e-05, 'epoch': 0.06}\n",
      "{'loss': 1.9537, 'learning_rate': 1.3162118780096307e-05, 'epoch': 0.07}\n",
      "{'loss': 1.9651, 'learning_rate': 1.348314606741573e-05, 'epoch': 0.07}\n",
      "{'loss': 1.9503, 'learning_rate': 1.3804173354735154e-05, 'epoch': 0.07}\n",
      "{'loss': 1.9809, 'learning_rate': 1.4125200642054575e-05, 'epoch': 0.07}\n",
      "{'loss': 1.9643, 'learning_rate': 1.4446227929373999e-05, 'epoch': 0.07}\n",
      "{'loss': 1.9127, 'learning_rate': 1.476725521669342e-05, 'epoch': 0.07}\n",
      "{'loss': 1.8629, 'learning_rate': 1.508828250401284e-05, 'epoch': 0.08}\n",
      "{'loss': 1.8918, 'learning_rate': 1.5409309791332264e-05, 'epoch': 0.08}\n",
      "{'loss': 1.8882, 'learning_rate': 1.5730337078651687e-05, 'epoch': 0.08}\n",
      "{'loss': 1.8519, 'learning_rate': 1.6051364365971107e-05, 'epoch': 0.08}\n",
      "{'loss': 1.9161, 'learning_rate': 1.637239165329053e-05, 'epoch': 0.08}\n",
      "{'loss': 1.9737, 'learning_rate': 1.6693418940609953e-05, 'epoch': 0.08}\n",
      "{'loss': 1.8396, 'learning_rate': 1.7014446227929377e-05, 'epoch': 0.08}\n",
      "{'loss': 1.8669, 'learning_rate': 1.7335473515248796e-05, 'epoch': 0.09}\n",
      "{'loss': 1.8349, 'learning_rate': 1.765650080256822e-05, 'epoch': 0.09}\n",
      "{'loss': 1.8607, 'learning_rate': 1.797752808988764e-05, 'epoch': 0.09}\n",
      "{'loss': 1.8074, 'learning_rate': 1.8298555377207063e-05, 'epoch': 0.09}\n",
      "{'loss': 1.9098, 'learning_rate': 1.8619582664526486e-05, 'epoch': 0.09}\n",
      "{'loss': 1.84, 'learning_rate': 1.894060995184591e-05, 'epoch': 0.09}\n",
      "{'loss': 1.821, 'learning_rate': 1.926163723916533e-05, 'epoch': 0.1}\n",
      "{'loss': 1.8459, 'learning_rate': 1.958266452648475e-05, 'epoch': 0.1}\n",
      "{'loss': 1.8096, 'learning_rate': 1.9903691813804173e-05, 'epoch': 0.1}\n",
      "{'loss': 1.8054, 'learning_rate': 2.0224719101123596e-05, 'epoch': 0.1}\n",
      "{'loss': 1.7573, 'learning_rate': 2.054574638844302e-05, 'epoch': 0.1}\n",
      "{'loss': 1.8286, 'learning_rate': 2.0866773675762442e-05, 'epoch': 0.1}\n",
      "{'loss': 1.7858, 'learning_rate': 2.1187800963081862e-05, 'epoch': 0.11}\n",
      "{'loss': 1.7155, 'learning_rate': 2.1508828250401286e-05, 'epoch': 0.11}\n",
      "{'loss': 1.8052, 'learning_rate': 2.182985553772071e-05, 'epoch': 0.11}\n",
      "{'loss': 1.7576, 'learning_rate': 2.215088282504013e-05, 'epoch': 0.11}\n",
      "{'loss': 1.7098, 'learning_rate': 2.2471910112359552e-05, 'epoch': 0.11}\n",
      "{'loss': 1.7322, 'learning_rate': 2.2792937399678972e-05, 'epoch': 0.11}\n",
      "{'loss': 1.7049, 'learning_rate': 2.3113964686998395e-05, 'epoch': 0.12}\n",
      "{'loss': 1.5744, 'learning_rate': 2.343499197431782e-05, 'epoch': 0.12}\n",
      "{'loss': 1.6931, 'learning_rate': 2.3756019261637242e-05, 'epoch': 0.12}\n",
      "{'loss': 1.6721, 'learning_rate': 2.4077046548956665e-05, 'epoch': 0.12}\n",
      "{'loss': 1.7433, 'learning_rate': 2.4398073836276085e-05, 'epoch': 0.12}\n",
      "{'loss': 1.692, 'learning_rate': 2.4719101123595505e-05, 'epoch': 0.12}\n",
      "{'loss': 1.612, 'learning_rate': 2.504012841091493e-05, 'epoch': 0.13}\n",
      "{'loss': 1.7367, 'learning_rate': 2.536115569823435e-05, 'epoch': 0.13}\n",
      "{'loss': 1.572, 'learning_rate': 2.568218298555377e-05, 'epoch': 0.13}\n",
      "{'loss': 1.6363, 'learning_rate': 2.6003210272873195e-05, 'epoch': 0.13}\n",
      "{'loss': 1.579, 'learning_rate': 2.6324237560192615e-05, 'epoch': 0.13}\n",
      "{'loss': 1.6357, 'learning_rate': 2.664526484751204e-05, 'epoch': 0.13}\n",
      "{'loss': 1.5869, 'learning_rate': 2.696629213483146e-05, 'epoch': 0.13}\n",
      "{'loss': 1.5309, 'learning_rate': 2.7287319422150888e-05, 'epoch': 0.14}\n",
      "{'loss': 1.6195, 'learning_rate': 2.7608346709470308e-05, 'epoch': 0.14}\n",
      "{'loss': 1.5736, 'learning_rate': 2.7929373996789728e-05, 'epoch': 0.14}\n",
      "{'loss': 1.6447, 'learning_rate': 2.825040128410915e-05, 'epoch': 0.14}\n",
      "{'loss': 1.6301, 'learning_rate': 2.857142857142857e-05, 'epoch': 0.14}\n",
      "{'loss': 1.5385, 'learning_rate': 2.8892455858747998e-05, 'epoch': 0.14}\n",
      "{'loss': 1.6218, 'learning_rate': 2.9213483146067417e-05, 'epoch': 0.15}\n",
      "{'loss': 1.561, 'learning_rate': 2.953451043338684e-05, 'epoch': 0.15}\n",
      "{'loss': 1.5353, 'learning_rate': 2.985553772070626e-05, 'epoch': 0.15}\n",
      "{'loss': 1.5129, 'learning_rate': 3.017656500802568e-05, 'epoch': 0.15}\n",
      "{'loss': 1.5368, 'learning_rate': 3.0497592295345107e-05, 'epoch': 0.15}\n",
      "{'loss': 1.543, 'learning_rate': 3.081861958266453e-05, 'epoch': 0.15}\n",
      "{'loss': 1.5311, 'learning_rate': 3.113964686998395e-05, 'epoch': 0.16}\n",
      "{'loss': 1.5135, 'learning_rate': 3.1460674157303374e-05, 'epoch': 0.16}\n",
      "{'loss': 1.5763, 'learning_rate': 3.17817014446228e-05, 'epoch': 0.16}\n",
      "{'loss': 1.5588, 'learning_rate': 3.2102728731942213e-05, 'epoch': 0.16}\n",
      "{'loss': 1.5673, 'learning_rate': 3.242375601926164e-05, 'epoch': 0.16}\n",
      "{'loss': 1.5302, 'learning_rate': 3.274478330658106e-05, 'epoch': 0.16}\n",
      "{'loss': 1.5256, 'learning_rate': 3.306581059390048e-05, 'epoch': 0.17}\n",
      "{'loss': 1.4925, 'learning_rate': 3.3386837881219907e-05, 'epoch': 0.17}\n",
      "{'loss': 1.5372, 'learning_rate': 3.370786516853933e-05, 'epoch': 0.17}\n",
      "{'loss': 1.5202, 'learning_rate': 3.402889245585875e-05, 'epoch': 0.17}\n",
      "{'loss': 1.4245, 'learning_rate': 3.434991974317817e-05, 'epoch': 0.17}\n",
      "{'loss': 1.4934, 'learning_rate': 3.467094703049759e-05, 'epoch': 0.17}\n",
      "{'loss': 1.5152, 'learning_rate': 3.4991974317817016e-05, 'epoch': 0.17}\n",
      "{'loss': 1.542, 'learning_rate': 3.531300160513644e-05, 'epoch': 0.18}\n",
      "{'loss': 1.474, 'learning_rate': 3.563402889245586e-05, 'epoch': 0.18}\n",
      "{'loss': 1.5078, 'learning_rate': 3.595505617977528e-05, 'epoch': 0.18}\n",
      "{'loss': 1.5725, 'learning_rate': 3.627608346709471e-05, 'epoch': 0.18}\n",
      "{'loss': 1.4646, 'learning_rate': 3.6597110754414126e-05, 'epoch': 0.18}\n",
      "{'loss': 1.4445, 'learning_rate': 3.691813804173355e-05, 'epoch': 0.18}\n",
      "{'loss': 1.5438, 'learning_rate': 3.723916532905297e-05, 'epoch': 0.19}\n",
      "{'loss': 1.4418, 'learning_rate': 3.756019261637239e-05, 'epoch': 0.19}\n",
      "{'loss': 1.4759, 'learning_rate': 3.788121990369182e-05, 'epoch': 0.19}\n",
      "{'loss': 1.4692, 'learning_rate': 3.8202247191011236e-05, 'epoch': 0.19}\n",
      "{'loss': 1.4051, 'learning_rate': 3.852327447833066e-05, 'epoch': 0.19}\n",
      "{'loss': 1.48, 'learning_rate': 3.884430176565008e-05, 'epoch': 0.19}\n",
      "{'loss': 1.4487, 'learning_rate': 3.91653290529695e-05, 'epoch': 0.2}\n",
      "{'loss': 1.4751, 'learning_rate': 3.948635634028893e-05, 'epoch': 0.2}\n",
      "{'loss': 1.4221, 'learning_rate': 3.9807383627608345e-05, 'epoch': 0.2}\n",
      "{'loss': 1.4401, 'learning_rate': 4.0128410914927775e-05, 'epoch': 0.2}\n",
      "{'loss': 1.404, 'learning_rate': 4.044943820224719e-05, 'epoch': 0.2}\n",
      "{'loss': 1.4335, 'learning_rate': 4.0770465489566615e-05, 'epoch': 0.2}\n",
      "{'loss': 1.4648, 'learning_rate': 4.109149277688604e-05, 'epoch': 0.21}\n",
      "{'loss': 1.4394, 'learning_rate': 4.1412520064205455e-05, 'epoch': 0.21}\n",
      "{'loss': 1.4337, 'learning_rate': 4.1733547351524885e-05, 'epoch': 0.21}\n",
      "{'loss': 1.4883, 'learning_rate': 4.20545746388443e-05, 'epoch': 0.21}\n",
      "{'loss': 1.4232, 'learning_rate': 4.2375601926163725e-05, 'epoch': 0.21}\n",
      "{'loss': 1.4406, 'learning_rate': 4.269662921348315e-05, 'epoch': 0.21}\n",
      "{'loss': 1.438, 'learning_rate': 4.301765650080257e-05, 'epoch': 0.21}\n",
      "{'loss': 1.4471, 'learning_rate': 4.3338683788121995e-05, 'epoch': 0.22}\n",
      "{'loss': 1.4356, 'learning_rate': 4.365971107544142e-05, 'epoch': 0.22}\n",
      "{'loss': 1.3892, 'learning_rate': 4.3980738362760834e-05, 'epoch': 0.22}\n",
      "{'loss': 1.3893, 'learning_rate': 4.430176565008026e-05, 'epoch': 0.22}\n",
      "{'loss': 1.4663, 'learning_rate': 4.462279293739968e-05, 'epoch': 0.22}\n",
      "{'loss': 1.3502, 'learning_rate': 4.4943820224719104e-05, 'epoch': 0.22}\n",
      "{'loss': 1.4521, 'learning_rate': 4.526484751203853e-05, 'epoch': 0.23}\n",
      "{'loss': 1.4434, 'learning_rate': 4.5585874799357944e-05, 'epoch': 0.23}\n",
      "{'loss': 1.4438, 'learning_rate': 4.5906902086677374e-05, 'epoch': 0.23}\n",
      "{'loss': 1.4439, 'learning_rate': 4.622792937399679e-05, 'epoch': 0.23}\n",
      "{'loss': 1.3885, 'learning_rate': 4.6548956661316214e-05, 'epoch': 0.23}\n",
      "{'loss': 1.3086, 'learning_rate': 4.686998394863564e-05, 'epoch': 0.23}\n",
      "{'loss': 1.4443, 'learning_rate': 4.719101123595506e-05, 'epoch': 0.24}\n",
      "{'loss': 1.4161, 'learning_rate': 4.7512038523274484e-05, 'epoch': 0.24}\n",
      "{'loss': 1.3058, 'learning_rate': 4.78330658105939e-05, 'epoch': 0.24}\n",
      "{'loss': 1.3761, 'learning_rate': 4.815409309791333e-05, 'epoch': 0.24}\n",
      "{'loss': 1.4052, 'learning_rate': 4.847512038523275e-05, 'epoch': 0.24}\n",
      "{'loss': 1.4146, 'learning_rate': 4.879614767255217e-05, 'epoch': 0.24}\n",
      "{'loss': 1.4042, 'learning_rate': 4.9117174959871593e-05, 'epoch': 0.25}\n",
      "{'loss': 1.3814, 'learning_rate': 4.943820224719101e-05, 'epoch': 0.25}\n",
      "{'loss': 1.3667, 'learning_rate': 4.975922953451044e-05, 'epoch': 0.25}\n",
      "{'loss': 1.3941, 'learning_rate': 5.008025682182986e-05, 'epoch': 0.25}\n",
      "{'loss': 1.4005, 'learning_rate': 5.040128410914928e-05, 'epoch': 0.25}\n",
      "{'loss': 1.3529, 'learning_rate': 5.07223113964687e-05, 'epoch': 0.25}\n",
      "{'loss': 1.4414, 'learning_rate': 5.104333868378812e-05, 'epoch': 0.25}\n",
      "{'loss': 1.3964, 'learning_rate': 5.136436597110754e-05, 'epoch': 0.26}\n",
      "{'loss': 1.4395, 'learning_rate': 5.168539325842697e-05, 'epoch': 0.26}\n",
      "{'loss': 1.3691, 'learning_rate': 5.200642054574639e-05, 'epoch': 0.26}\n",
      "{'loss': 1.3569, 'learning_rate': 5.232744783306581e-05, 'epoch': 0.26}\n",
      "{'loss': 1.4183, 'learning_rate': 5.264847512038523e-05, 'epoch': 0.26}\n",
      "{'loss': 1.426, 'learning_rate': 5.296950240770465e-05, 'epoch': 0.26}\n",
      "{'loss': 1.3737, 'learning_rate': 5.329052969502408e-05, 'epoch': 0.27}\n",
      "{'loss': 1.4103, 'learning_rate': 5.3611556982343506e-05, 'epoch': 0.27}\n",
      "{'loss': 1.364, 'learning_rate': 5.393258426966292e-05, 'epoch': 0.27}\n",
      "{'loss': 1.3524, 'learning_rate': 5.425361155698234e-05, 'epoch': 0.27}\n",
      "{'loss': 1.3877, 'learning_rate': 5.4574638844301776e-05, 'epoch': 0.27}\n",
      "{'loss': 1.3285, 'learning_rate': 5.489566613162119e-05, 'epoch': 0.27}\n",
      "{'loss': 1.3202, 'learning_rate': 5.5216693418940616e-05, 'epoch': 0.28}\n",
      "{'loss': 1.4062, 'learning_rate': 5.553772070626003e-05, 'epoch': 0.28}\n",
      "{'loss': 1.414, 'learning_rate': 5.5858747993579455e-05, 'epoch': 0.28}\n",
      "{'loss': 1.3864, 'learning_rate': 5.6179775280898885e-05, 'epoch': 0.28}\n",
      "{'loss': 1.3975, 'learning_rate': 5.65008025682183e-05, 'epoch': 0.28}\n",
      "{'loss': 1.3346, 'learning_rate': 5.6821829855537725e-05, 'epoch': 0.28}\n",
      "{'loss': 1.3377, 'learning_rate': 5.714285714285714e-05, 'epoch': 0.29}\n",
      "{'loss': 1.3412, 'learning_rate': 5.746388443017657e-05, 'epoch': 0.29}\n",
      "{'loss': 1.385, 'learning_rate': 5.7784911717495995e-05, 'epoch': 0.29}\n",
      "{'loss': 1.308, 'learning_rate': 5.810593900481541e-05, 'epoch': 0.29}\n",
      "{'loss': 1.3331, 'learning_rate': 5.8426966292134835e-05, 'epoch': 0.29}\n",
      "{'loss': 1.4036, 'learning_rate': 5.874799357945425e-05, 'epoch': 0.29}\n",
      "{'loss': 1.3963, 'learning_rate': 5.906902086677368e-05, 'epoch': 0.29}\n",
      "{'loss': 1.3869, 'learning_rate': 5.9390048154093105e-05, 'epoch': 0.3}\n",
      "{'loss': 1.3791, 'learning_rate': 5.971107544141252e-05, 'epoch': 0.3}\n",
      "{'loss': 1.3297, 'learning_rate': 6.0032102728731945e-05, 'epoch': 0.3}\n",
      "{'loss': 1.3156, 'learning_rate': 6.035313001605136e-05, 'epoch': 0.3}\n",
      "{'loss': 1.3917, 'learning_rate': 6.067415730337079e-05, 'epoch': 0.3}\n",
      "{'loss': 1.4184, 'learning_rate': 6.0995184590690214e-05, 'epoch': 0.3}\n",
      "{'loss': 1.3248, 'learning_rate': 6.131621187800964e-05, 'epoch': 0.31}\n",
      "{'loss': 1.3278, 'learning_rate': 6.163723916532905e-05, 'epoch': 0.31}\n",
      "{'loss': 1.33, 'learning_rate': 6.195826645264848e-05, 'epoch': 0.31}\n",
      "{'loss': 1.4239, 'learning_rate': 6.22792937399679e-05, 'epoch': 0.31}\n",
      "{'loss': 1.3298, 'learning_rate': 6.260032102728732e-05, 'epoch': 0.31}\n",
      "{'loss': 1.3819, 'learning_rate': 6.292134831460675e-05, 'epoch': 0.31}\n",
      "{'loss': 1.3312, 'learning_rate': 6.324237560192616e-05, 'epoch': 0.32}\n",
      "{'loss': 1.3231, 'learning_rate': 6.35634028892456e-05, 'epoch': 0.32}\n",
      "{'loss': 1.3608, 'learning_rate': 6.388443017656501e-05, 'epoch': 0.32}\n",
      "{'loss': 1.3418, 'learning_rate': 6.420545746388443e-05, 'epoch': 0.32}\n",
      "{'loss': 1.327, 'learning_rate': 6.452648475120386e-05, 'epoch': 0.32}\n",
      "{'loss': 1.3422, 'learning_rate': 6.484751203852327e-05, 'epoch': 0.32}\n",
      "{'loss': 1.3265, 'learning_rate': 6.51685393258427e-05, 'epoch': 0.33}\n",
      "{'loss': 1.2805, 'learning_rate': 6.548956661316212e-05, 'epoch': 0.33}\n",
      "{'loss': 1.3541, 'learning_rate': 6.581059390048154e-05, 'epoch': 0.33}\n",
      "{'loss': 1.3982, 'learning_rate': 6.613162118780097e-05, 'epoch': 0.33}\n",
      "{'loss': 1.3674, 'learning_rate': 6.64526484751204e-05, 'epoch': 0.33}\n",
      "{'loss': 1.3555, 'learning_rate': 6.677367576243981e-05, 'epoch': 0.33}\n",
      "{'loss': 1.4153, 'learning_rate': 6.709470304975923e-05, 'epoch': 0.34}\n",
      "{'loss': 1.403, 'learning_rate': 6.741573033707866e-05, 'epoch': 0.34}\n",
      "{'loss': 1.386, 'learning_rate': 6.773675762439808e-05, 'epoch': 0.34}\n",
      "{'loss': 1.3904, 'learning_rate': 6.80577849117175e-05, 'epoch': 0.34}\n",
      "{'loss': 1.3418, 'learning_rate': 6.837881219903692e-05, 'epoch': 0.34}\n",
      "{'loss': 1.3663, 'learning_rate': 6.869983948635634e-05, 'epoch': 0.34}\n",
      "{'loss': 1.3479, 'learning_rate': 6.902086677367577e-05, 'epoch': 0.34}\n",
      "{'loss': 1.3784, 'learning_rate': 6.934189406099519e-05, 'epoch': 0.35}\n",
      "{'loss': 1.3757, 'learning_rate': 6.966292134831462e-05, 'epoch': 0.35}\n",
      "{'loss': 1.3327, 'learning_rate': 6.998394863563403e-05, 'epoch': 0.35}\n",
      "{'loss': 1.274, 'learning_rate': 7.030497592295345e-05, 'epoch': 0.35}\n",
      "{'loss': 1.3437, 'learning_rate': 7.062600321027288e-05, 'epoch': 0.35}\n",
      "{'loss': 1.3334, 'learning_rate': 7.094703049759231e-05, 'epoch': 0.35}\n",
      "{'loss': 1.3295, 'learning_rate': 7.126805778491173e-05, 'epoch': 0.36}\n",
      "{'loss': 1.2867, 'learning_rate': 7.158908507223114e-05, 'epoch': 0.36}\n",
      "{'loss': 1.354, 'learning_rate': 7.191011235955056e-05, 'epoch': 0.36}\n",
      "{'loss': 1.3541, 'learning_rate': 7.223113964686999e-05, 'epoch': 0.36}\n",
      "{'loss': 1.3179, 'learning_rate': 7.255216693418942e-05, 'epoch': 0.36}\n",
      "{'loss': 1.3707, 'learning_rate': 7.287319422150884e-05, 'epoch': 0.36}\n",
      "{'loss': 1.3499, 'learning_rate': 7.319422150882825e-05, 'epoch': 0.37}\n",
      "{'loss': 1.3794, 'learning_rate': 7.351524879614767e-05, 'epoch': 0.37}\n",
      "{'loss': 1.3112, 'learning_rate': 7.38362760834671e-05, 'epoch': 0.37}\n",
      "{'loss': 1.3843, 'learning_rate': 7.415730337078653e-05, 'epoch': 0.37}\n",
      "{'loss': 1.3396, 'learning_rate': 7.447833065810594e-05, 'epoch': 0.37}\n",
      "{'loss': 1.3383, 'learning_rate': 7.479935794542536e-05, 'epoch': 0.37}\n",
      "{'loss': 1.4002, 'learning_rate': 7.512038523274478e-05, 'epoch': 0.38}\n",
      "{'loss': 1.4072, 'learning_rate': 7.544141252006421e-05, 'epoch': 0.38}\n",
      "{'loss': 1.3405, 'learning_rate': 7.576243980738364e-05, 'epoch': 0.38}\n",
      "{'loss': 1.3748, 'learning_rate': 7.608346709470305e-05, 'epoch': 0.38}\n",
      "{'loss': 1.3313, 'learning_rate': 7.640449438202247e-05, 'epoch': 0.38}\n",
      "{'loss': 1.2513, 'learning_rate': 7.672552166934189e-05, 'epoch': 0.38}\n",
      "{'loss': 1.3891, 'learning_rate': 7.704654895666132e-05, 'epoch': 0.38}\n",
      "{'loss': 1.352, 'learning_rate': 7.736757624398075e-05, 'epoch': 0.39}\n",
      "{'loss': 1.3529, 'learning_rate': 7.768860353130016e-05, 'epoch': 0.39}\n",
      "{'loss': 1.4263, 'learning_rate': 7.800963081861958e-05, 'epoch': 0.39}\n",
      "{'loss': 1.3592, 'learning_rate': 7.8330658105939e-05, 'epoch': 0.39}\n",
      "{'loss': 1.3223, 'learning_rate': 7.865168539325843e-05, 'epoch': 0.39}\n",
      "{'loss': 1.2924, 'learning_rate': 7.897271268057786e-05, 'epoch': 0.39}\n",
      "{'loss': 1.2948, 'learning_rate': 7.929373996789727e-05, 'epoch': 0.4}\n",
      "{'loss': 1.3433, 'learning_rate': 7.961476725521669e-05, 'epoch': 0.4}\n",
      "{'loss': 1.3583, 'learning_rate': 7.993579454253612e-05, 'epoch': 0.4}\n",
      "{'loss': 1.3228, 'learning_rate': 8.025682182985555e-05, 'epoch': 0.4}\n",
      "{'loss': 1.2932, 'learning_rate': 8.057784911717497e-05, 'epoch': 0.4}\n",
      "{'loss': 1.2984, 'learning_rate': 8.089887640449438e-05, 'epoch': 0.4}\n",
      "{'loss': 1.304, 'learning_rate': 8.12199036918138e-05, 'epoch': 0.41}\n",
      "{'loss': 1.2773, 'learning_rate': 8.154093097913323e-05, 'epoch': 0.41}\n",
      "{'loss': 1.2752, 'learning_rate': 8.186195826645266e-05, 'epoch': 0.41}\n",
      "{'loss': 1.3276, 'learning_rate': 8.218298555377208e-05, 'epoch': 0.41}\n",
      "{'loss': 1.3296, 'learning_rate': 8.25040128410915e-05, 'epoch': 0.41}\n",
      "{'loss': 1.3107, 'learning_rate': 8.282504012841091e-05, 'epoch': 0.41}\n",
      "{'loss': 1.3731, 'learning_rate': 8.314606741573034e-05, 'epoch': 0.42}\n",
      "{'loss': 1.329, 'learning_rate': 8.346709470304977e-05, 'epoch': 0.42}\n",
      "{'loss': 1.3608, 'learning_rate': 8.378812199036919e-05, 'epoch': 0.42}\n",
      "{'loss': 1.3978, 'learning_rate': 8.41091492776886e-05, 'epoch': 0.42}\n",
      "{'loss': 1.2555, 'learning_rate': 8.443017656500803e-05, 'epoch': 0.42}\n",
      "{'loss': 1.2743, 'learning_rate': 8.475120385232745e-05, 'epoch': 0.42}\n",
      "{'loss': 1.272, 'learning_rate': 8.507223113964688e-05, 'epoch': 0.42}\n",
      "{'loss': 1.2143, 'learning_rate': 8.53932584269663e-05, 'epoch': 0.43}\n",
      "{'loss': 1.31, 'learning_rate': 8.571428571428571e-05, 'epoch': 0.43}\n",
      "{'loss': 1.245, 'learning_rate': 8.603531300160514e-05, 'epoch': 0.43}\n",
      "{'loss': 1.2538, 'learning_rate': 8.635634028892456e-05, 'epoch': 0.43}\n",
      "{'loss': 1.3268, 'learning_rate': 8.667736757624399e-05, 'epoch': 0.43}\n",
      "{'loss': 1.3395, 'learning_rate': 8.69983948635634e-05, 'epoch': 0.43}\n",
      "{'loss': 1.2755, 'learning_rate': 8.731942215088284e-05, 'epoch': 0.44}\n",
      "{'loss': 1.2868, 'learning_rate': 8.764044943820225e-05, 'epoch': 0.44}\n",
      "{'loss': 1.3113, 'learning_rate': 8.796147672552167e-05, 'epoch': 0.44}\n",
      "{'loss': 1.2844, 'learning_rate': 8.82825040128411e-05, 'epoch': 0.44}\n",
      "{'loss': 1.2663, 'learning_rate': 8.860353130016052e-05, 'epoch': 0.44}\n",
      "{'loss': 1.2905, 'learning_rate': 8.892455858747995e-05, 'epoch': 0.44}\n",
      "{'loss': 1.3154, 'learning_rate': 8.924558587479936e-05, 'epoch': 0.45}\n",
      "{'loss': 1.2585, 'learning_rate': 8.956661316211878e-05, 'epoch': 0.45}\n",
      "{'loss': 1.2581, 'learning_rate': 8.988764044943821e-05, 'epoch': 0.45}\n",
      "{'loss': 1.2962, 'learning_rate': 9.020866773675763e-05, 'epoch': 0.45}\n",
      "{'loss': 1.2543, 'learning_rate': 9.052969502407706e-05, 'epoch': 0.45}\n",
      "{'loss': 1.3283, 'learning_rate': 9.085072231139647e-05, 'epoch': 0.45}\n",
      "{'loss': 1.33, 'learning_rate': 9.117174959871589e-05, 'epoch': 0.46}\n",
      "{'loss': 1.3099, 'learning_rate': 9.149277688603532e-05, 'epoch': 0.46}\n",
      "{'loss': 1.3267, 'learning_rate': 9.181380417335475e-05, 'epoch': 0.46}\n",
      "{'loss': 1.2559, 'learning_rate': 9.213483146067416e-05, 'epoch': 0.46}\n",
      "{'loss': 1.2603, 'learning_rate': 9.245585874799358e-05, 'epoch': 0.46}\n",
      "{'loss': 1.3423, 'learning_rate': 9.2776886035313e-05, 'epoch': 0.46}\n",
      "{'loss': 1.2286, 'learning_rate': 9.309791332263243e-05, 'epoch': 0.46}\n",
      "{'loss': 1.2573, 'learning_rate': 9.341894060995186e-05, 'epoch': 0.47}\n",
      "{'loss': 1.3193, 'learning_rate': 9.373996789727127e-05, 'epoch': 0.47}\n",
      "{'loss': 1.2462, 'learning_rate': 9.406099518459069e-05, 'epoch': 0.47}\n",
      "{'loss': 1.251, 'learning_rate': 9.438202247191012e-05, 'epoch': 0.47}\n",
      "{'loss': 1.3569, 'learning_rate': 9.470304975922954e-05, 'epoch': 0.47}\n",
      "{'loss': 1.2626, 'learning_rate': 9.502407704654897e-05, 'epoch': 0.47}\n",
      "{'loss': 1.2649, 'learning_rate': 9.534510433386838e-05, 'epoch': 0.48}\n",
      "{'loss': 1.2879, 'learning_rate': 9.56661316211878e-05, 'epoch': 0.48}\n",
      "{'loss': 1.326, 'learning_rate': 9.598715890850723e-05, 'epoch': 0.48}\n",
      "{'loss': 1.1994, 'learning_rate': 9.630818619582666e-05, 'epoch': 0.48}\n",
      "{'loss': 1.2875, 'learning_rate': 9.662921348314608e-05, 'epoch': 0.48}\n",
      "{'loss': 1.322, 'learning_rate': 9.69502407704655e-05, 'epoch': 0.48}\n",
      "{'loss': 1.3307, 'learning_rate': 9.727126805778491e-05, 'epoch': 0.49}\n",
      "{'loss': 1.2504, 'learning_rate': 9.759229534510434e-05, 'epoch': 0.49}\n",
      "{'loss': 1.2586, 'learning_rate': 9.791332263242377e-05, 'epoch': 0.49}\n",
      "{'loss': 1.1936, 'learning_rate': 9.823434991974319e-05, 'epoch': 0.49}\n",
      "{'loss': 1.2535, 'learning_rate': 9.85553772070626e-05, 'epoch': 0.49}\n",
      "{'loss': 1.2673, 'learning_rate': 9.887640449438202e-05, 'epoch': 0.49}\n",
      "{'loss': 1.2256, 'learning_rate': 9.919743178170145e-05, 'epoch': 0.5}\n",
      "{'loss': 1.2711, 'learning_rate': 9.951845906902088e-05, 'epoch': 0.5}\n",
      "{'loss': 1.2372, 'learning_rate': 9.98394863563403e-05, 'epoch': 0.5}\n",
      "{'loss': 1.2933, 'learning_rate': 0.00010016051364365973, 'epoch': 0.5}\n",
      "{'loss': 1.2686, 'learning_rate': 0.00010048154093097914, 'epoch': 0.5}\n",
      "{'loss': 1.2632, 'learning_rate': 0.00010080256821829856, 'epoch': 0.5}\n",
      "{'loss': 1.2823, 'learning_rate': 0.00010112359550561799, 'epoch': 0.5}\n",
      "{'loss': 1.2627, 'learning_rate': 0.0001014446227929374, 'epoch': 0.51}\n",
      "{'loss': 1.2598, 'learning_rate': 0.00010176565008025682, 'epoch': 0.51}\n",
      "{'loss': 1.2041, 'learning_rate': 0.00010208667736757624, 'epoch': 0.51}\n",
      "{'loss': 1.2662, 'learning_rate': 0.00010240770465489567, 'epoch': 0.51}\n",
      "{'loss': 1.3004, 'learning_rate': 0.00010272873194221509, 'epoch': 0.51}\n",
      "{'loss': 1.2896, 'learning_rate': 0.00010304975922953453, 'epoch': 0.51}\n",
      "{'loss': 1.2976, 'learning_rate': 0.00010337078651685395, 'epoch': 0.52}\n",
      "{'loss': 1.3075, 'learning_rate': 0.00010369181380417336, 'epoch': 0.52}\n",
      "{'loss': 1.2323, 'learning_rate': 0.00010401284109149278, 'epoch': 0.52}\n",
      "{'loss': 1.1904, 'learning_rate': 0.00010433386837881221, 'epoch': 0.52}\n",
      "{'loss': 1.2591, 'learning_rate': 0.00010465489566613163, 'epoch': 0.52}\n",
      "{'loss': 1.2879, 'learning_rate': 0.00010497592295345104, 'epoch': 0.52}\n",
      "{'loss': 1.2635, 'learning_rate': 0.00010529695024077046, 'epoch': 0.53}\n",
      "{'loss': 1.2734, 'learning_rate': 0.00010561797752808989, 'epoch': 0.53}\n",
      "{'loss': 1.2978, 'learning_rate': 0.0001059390048154093, 'epoch': 0.53}\n",
      "{'loss': 1.2506, 'learning_rate': 0.00010626003210272875, 'epoch': 0.53}\n",
      "{'loss': 1.324, 'learning_rate': 0.00010658105939004817, 'epoch': 0.53}\n",
      "{'loss': 1.285, 'learning_rate': 0.00010690208667736758, 'epoch': 0.53}\n",
      "{'loss': 1.2446, 'learning_rate': 0.00010722311396468701, 'epoch': 0.54}\n",
      "{'loss': 1.2799, 'learning_rate': 0.00010754414125200643, 'epoch': 0.54}\n",
      "{'loss': 1.2717, 'learning_rate': 0.00010786516853932584, 'epoch': 0.54}\n",
      "{'loss': 1.2098, 'learning_rate': 0.00010818619582664526, 'epoch': 0.54}\n",
      "{'loss': 1.2542, 'learning_rate': 0.00010850722311396468, 'epoch': 0.54}\n",
      "{'loss': 1.3093, 'learning_rate': 0.00010882825040128411, 'epoch': 0.54}\n",
      "{'loss': 1.306, 'learning_rate': 0.00010914927768860355, 'epoch': 0.55}\n",
      "{'loss': 1.3693, 'learning_rate': 0.00010947030497592297, 'epoch': 0.55}\n",
      "{'loss': 1.2484, 'learning_rate': 0.00010979133226324238, 'epoch': 0.55}\n",
      "{'loss': 1.2889, 'learning_rate': 0.0001101123595505618, 'epoch': 0.55}\n",
      "{'loss': 1.2227, 'learning_rate': 0.00011043338683788123, 'epoch': 0.55}\n",
      "{'loss': 1.2957, 'learning_rate': 0.00011075441412520065, 'epoch': 0.55}\n",
      "{'loss': 1.2563, 'learning_rate': 0.00011107544141252006, 'epoch': 0.55}\n",
      "{'loss': 1.2652, 'learning_rate': 0.00011139646869983948, 'epoch': 0.56}\n",
      "{'loss': 1.2992, 'learning_rate': 0.00011171749598715891, 'epoch': 0.56}\n",
      "{'loss': 1.199, 'learning_rate': 0.00011203852327447834, 'epoch': 0.56}\n",
      "{'loss': 1.2991, 'learning_rate': 0.00011235955056179777, 'epoch': 0.56}\n",
      "{'loss': 1.2436, 'learning_rate': 0.00011268057784911719, 'epoch': 0.56}\n",
      "{'loss': 1.2875, 'learning_rate': 0.0001130016051364366, 'epoch': 0.56}\n",
      "{'loss': 1.2766, 'learning_rate': 0.00011332263242375602, 'epoch': 0.57}\n",
      "{'loss': 1.2565, 'learning_rate': 0.00011364365971107545, 'epoch': 0.57}\n",
      "{'loss': 1.2977, 'learning_rate': 0.00011396468699839487, 'epoch': 0.57}\n",
      "{'loss': 1.2622, 'learning_rate': 0.00011428571428571428, 'epoch': 0.57}\n",
      "{'loss': 1.2411, 'learning_rate': 0.0001146067415730337, 'epoch': 0.57}\n",
      "{'loss': 1.3211, 'learning_rate': 0.00011492776886035314, 'epoch': 0.57}\n",
      "{'loss': 1.1695, 'learning_rate': 0.00011524879614767256, 'epoch': 0.58}\n",
      "{'loss': 1.2945, 'learning_rate': 0.00011556982343499199, 'epoch': 0.58}\n",
      "{'loss': 1.2813, 'learning_rate': 0.0001158908507223114, 'epoch': 0.58}\n",
      "{'loss': 1.2776, 'learning_rate': 0.00011621187800963082, 'epoch': 0.58}\n",
      "{'loss': 1.2469, 'learning_rate': 0.00011653290529695024, 'epoch': 0.58}\n",
      "{'loss': 1.2889, 'learning_rate': 0.00011685393258426967, 'epoch': 0.58}\n",
      "{'loss': 1.3889, 'learning_rate': 0.00011717495987158909, 'epoch': 0.59}\n",
      "{'loss': 1.3159, 'learning_rate': 0.0001174959871589085, 'epoch': 0.59}\n",
      "{'loss': 1.2099, 'learning_rate': 0.00011781701444622792, 'epoch': 0.59}\n",
      "{'loss': 1.2485, 'learning_rate': 0.00011813804173354736, 'epoch': 0.59}\n",
      "{'loss': 1.2732, 'learning_rate': 0.00011845906902086679, 'epoch': 0.59}\n",
      "{'loss': 1.2413, 'learning_rate': 0.00011878009630818621, 'epoch': 0.59}\n",
      "{'loss': 1.3206, 'learning_rate': 0.00011910112359550563, 'epoch': 0.59}\n",
      "{'loss': 1.3193, 'learning_rate': 0.00011942215088282504, 'epoch': 0.6}\n",
      "{'loss': 1.2332, 'learning_rate': 0.00011974317817014446, 'epoch': 0.6}\n",
      "{'loss': 1.2973, 'learning_rate': 0.00012006420545746389, 'epoch': 0.6}\n",
      "{'loss': 1.3217, 'learning_rate': 0.0001203852327447833, 'epoch': 0.6}\n",
      "{'loss': 1.2671, 'learning_rate': 0.00012070626003210272, 'epoch': 0.6}\n",
      "{'loss': 1.2703, 'learning_rate': 0.00012102728731942217, 'epoch': 0.6}\n",
      "{'loss': 1.246, 'learning_rate': 0.00012134831460674158, 'epoch': 0.61}\n",
      "{'loss': 1.3062, 'learning_rate': 0.00012166934189406101, 'epoch': 0.61}\n",
      "{'loss': 1.2414, 'learning_rate': 0.00012199036918138043, 'epoch': 0.61}\n",
      "{'loss': 1.2875, 'learning_rate': 0.00012231139646869986, 'epoch': 0.61}\n",
      "{'loss': 1.2604, 'learning_rate': 0.00012263242375601928, 'epoch': 0.61}\n",
      "{'loss': 1.2657, 'learning_rate': 0.0001229534510433387, 'epoch': 0.61}\n",
      "{'loss': 1.3058, 'learning_rate': 0.0001232744783306581, 'epoch': 0.62}\n",
      "{'loss': 1.2689, 'learning_rate': 0.00012359550561797752, 'epoch': 0.62}\n",
      "{'loss': 1.2046, 'learning_rate': 0.00012391653290529697, 'epoch': 0.62}\n",
      "{'loss': 1.2327, 'learning_rate': 0.00012423756019261639, 'epoch': 0.62}\n",
      "{'loss': 1.2491, 'learning_rate': 0.0001245585874799358, 'epoch': 0.62}\n",
      "{'loss': 1.2502, 'learning_rate': 0.00012487961476725522, 'epoch': 0.62}\n",
      "{'loss': 1.2407, 'learning_rate': 0.00012520064205457463, 'epoch': 0.63}\n",
      "{'loss': 1.2366, 'learning_rate': 0.00012552166934189408, 'epoch': 0.63}\n",
      "{'loss': 1.2477, 'learning_rate': 0.0001258426966292135, 'epoch': 0.63}\n",
      "{'loss': 1.263, 'learning_rate': 0.0001261637239165329, 'epoch': 0.63}\n",
      "{'loss': 1.2728, 'learning_rate': 0.00012648475120385233, 'epoch': 0.63}\n",
      "{'loss': 1.2781, 'learning_rate': 0.00012680577849117174, 'epoch': 0.63}\n",
      "{'loss': 1.3179, 'learning_rate': 0.0001271268057784912, 'epoch': 0.63}\n",
      "{'loss': 1.2251, 'learning_rate': 0.0001274478330658106, 'epoch': 0.64}\n",
      "{'loss': 1.2669, 'learning_rate': 0.00012776886035313002, 'epoch': 0.64}\n",
      "{'loss': 1.2264, 'learning_rate': 0.00012808988764044944, 'epoch': 0.64}\n",
      "{'loss': 1.265, 'learning_rate': 0.00012841091492776885, 'epoch': 0.64}\n",
      "{'loss': 1.2703, 'learning_rate': 0.0001287319422150883, 'epoch': 0.64}\n",
      "{'loss': 1.2301, 'learning_rate': 0.00012905296950240771, 'epoch': 0.64}\n",
      "{'loss': 1.2754, 'learning_rate': 0.00012937399678972713, 'epoch': 0.65}\n",
      "{'loss': 1.2424, 'learning_rate': 0.00012969502407704655, 'epoch': 0.65}\n",
      "{'loss': 1.2252, 'learning_rate': 0.000130016051364366, 'epoch': 0.65}\n",
      "{'loss': 1.2855, 'learning_rate': 0.0001303370786516854, 'epoch': 0.65}\n",
      "{'loss': 1.256, 'learning_rate': 0.00013065810593900482, 'epoch': 0.65}\n",
      "{'loss': 1.2584, 'learning_rate': 0.00013097913322632424, 'epoch': 0.65}\n",
      "{'loss': 1.2876, 'learning_rate': 0.00013130016051364366, 'epoch': 0.66}\n",
      "{'loss': 1.2696, 'learning_rate': 0.00013162118780096307, 'epoch': 0.66}\n",
      "{'loss': 1.1902, 'learning_rate': 0.00013194221508828252, 'epoch': 0.66}\n",
      "{'loss': 1.2538, 'learning_rate': 0.00013226324237560193, 'epoch': 0.66}\n",
      "{'loss': 1.3075, 'learning_rate': 0.00013258426966292135, 'epoch': 0.66}\n",
      "{'loss': 1.2735, 'learning_rate': 0.0001329052969502408, 'epoch': 0.66}\n",
      "{'loss': 1.2606, 'learning_rate': 0.0001332263242375602, 'epoch': 0.67}\n",
      "{'loss': 1.3463, 'learning_rate': 0.00013354735152487963, 'epoch': 0.67}\n",
      "{'loss': 1.2821, 'learning_rate': 0.00013386837881219904, 'epoch': 0.67}\n",
      "{'loss': 1.2761, 'learning_rate': 0.00013418940609951846, 'epoch': 0.67}\n",
      "{'loss': 1.291, 'learning_rate': 0.00013451043338683788, 'epoch': 0.67}\n",
      "{'loss': 1.2604, 'learning_rate': 0.00013483146067415732, 'epoch': 0.67}\n",
      "{'loss': 1.3065, 'learning_rate': 0.00013515248796147674, 'epoch': 0.67}\n",
      "{'loss': 1.2235, 'learning_rate': 0.00013547351524879615, 'epoch': 0.68}\n",
      "{'loss': 1.2155, 'learning_rate': 0.00013579454253611557, 'epoch': 0.68}\n",
      "{'loss': 1.2134, 'learning_rate': 0.000136115569823435, 'epoch': 0.68}\n",
      "{'loss': 1.2925, 'learning_rate': 0.00013643659711075443, 'epoch': 0.68}\n",
      "{'loss': 1.3071, 'learning_rate': 0.00013675762439807385, 'epoch': 0.68}\n",
      "{'loss': 1.2433, 'learning_rate': 0.00013707865168539326, 'epoch': 0.68}\n",
      "{'loss': 1.3033, 'learning_rate': 0.00013739967897271268, 'epoch': 0.69}\n",
      "{'loss': 1.2876, 'learning_rate': 0.0001377207062600321, 'epoch': 0.69}\n",
      "{'loss': 1.2647, 'learning_rate': 0.00013804173354735154, 'epoch': 0.69}\n",
      "{'loss': 1.2535, 'learning_rate': 0.00013836276083467096, 'epoch': 0.69}\n",
      "{'loss': 1.2294, 'learning_rate': 0.00013868378812199037, 'epoch': 0.69}\n",
      "{'loss': 1.241, 'learning_rate': 0.00013900481540930982, 'epoch': 0.69}\n",
      "{'loss': 1.2544, 'learning_rate': 0.00013932584269662923, 'epoch': 0.7}\n",
      "{'loss': 1.2435, 'learning_rate': 0.00013964686998394865, 'epoch': 0.7}\n",
      "{'loss': 1.1667, 'learning_rate': 0.00013996789727126807, 'epoch': 0.7}\n",
      "{'loss': 1.2403, 'learning_rate': 0.00014028892455858748, 'epoch': 0.7}\n",
      "{'loss': 1.2937, 'learning_rate': 0.0001406099518459069, 'epoch': 0.7}\n",
      "{'loss': 1.2637, 'learning_rate': 0.00014093097913322631, 'epoch': 0.7}\n",
      "{'loss': 1.2947, 'learning_rate': 0.00014125200642054576, 'epoch': 0.71}\n",
      "{'loss': 1.2171, 'learning_rate': 0.00014157303370786517, 'epoch': 0.71}\n",
      "{'loss': 1.3148, 'learning_rate': 0.00014189406099518462, 'epoch': 0.71}\n",
      "{'loss': 1.2813, 'learning_rate': 0.00014221508828250403, 'epoch': 0.71}\n",
      "{'loss': 1.2284, 'learning_rate': 0.00014253611556982345, 'epoch': 0.71}\n",
      "{'loss': 1.2563, 'learning_rate': 0.00014285714285714287, 'epoch': 0.71}\n",
      "{'loss': 1.2254, 'learning_rate': 0.00014317817014446228, 'epoch': 0.71}\n",
      "{'loss': 1.2552, 'learning_rate': 0.0001434991974317817, 'epoch': 0.72}\n",
      "{'loss': 1.2784, 'learning_rate': 0.00014382022471910112, 'epoch': 0.72}\n",
      "{'loss': 1.2198, 'learning_rate': 0.00014414125200642053, 'epoch': 0.72}\n",
      "{'loss': 1.3098, 'learning_rate': 0.00014446227929373998, 'epoch': 0.72}\n",
      "{'loss': 1.2752, 'learning_rate': 0.00014478330658105942, 'epoch': 0.72}\n",
      "{'loss': 1.2565, 'learning_rate': 0.00014510433386837884, 'epoch': 0.72}\n",
      "{'loss': 1.3178, 'learning_rate': 0.00014542536115569825, 'epoch': 0.73}\n",
      "{'loss': 1.1975, 'learning_rate': 0.00014574638844301767, 'epoch': 0.73}\n",
      "{'loss': 1.2685, 'learning_rate': 0.0001460674157303371, 'epoch': 0.73}\n",
      "{'loss': 1.1818, 'learning_rate': 0.0001463884430176565, 'epoch': 0.73}\n",
      "{'loss': 1.2105, 'learning_rate': 0.00014670947030497592, 'epoch': 0.73}\n",
      "{'loss': 1.2519, 'learning_rate': 0.00014703049759229534, 'epoch': 0.73}\n",
      "{'loss': 1.264, 'learning_rate': 0.00014735152487961475, 'epoch': 0.74}\n",
      "{'loss': 1.2495, 'learning_rate': 0.0001476725521669342, 'epoch': 0.74}\n",
      "{'loss': 1.2456, 'learning_rate': 0.00014799357945425364, 'epoch': 0.74}\n",
      "{'loss': 1.2651, 'learning_rate': 0.00014831460674157306, 'epoch': 0.74}\n",
      "{'loss': 1.263, 'learning_rate': 0.00014863563402889247, 'epoch': 0.74}\n",
      "{'loss': 1.2443, 'learning_rate': 0.0001489566613162119, 'epoch': 0.74}\n",
      "{'loss': 1.2592, 'learning_rate': 0.0001492776886035313, 'epoch': 0.75}\n",
      "{'loss': 1.1914, 'learning_rate': 0.00014959871589085072, 'epoch': 0.75}\n",
      "{'loss': 1.249, 'learning_rate': 0.00014991974317817014, 'epoch': 0.75}\n",
      "{'loss': 1.259, 'learning_rate': 0.00015024077046548956, 'epoch': 0.75}\n",
      "{'loss': 1.2385, 'learning_rate': 0.000150561797752809, 'epoch': 0.75}\n",
      "{'loss': 1.2595, 'learning_rate': 0.00015088282504012842, 'epoch': 0.75}\n",
      "{'loss': 1.2527, 'learning_rate': 0.00015120385232744786, 'epoch': 0.76}\n",
      "{'loss': 1.3005, 'learning_rate': 0.00015152487961476728, 'epoch': 0.76}\n",
      "{'loss': 1.2241, 'learning_rate': 0.0001518459069020867, 'epoch': 0.76}\n",
      "{'loss': 1.2238, 'learning_rate': 0.0001521669341894061, 'epoch': 0.76}\n",
      "{'loss': 1.3631, 'learning_rate': 0.00015248796147672553, 'epoch': 0.76}\n",
      "{'loss': 1.2227, 'learning_rate': 0.00015280898876404494, 'epoch': 0.76}\n",
      "{'loss': 1.2107, 'learning_rate': 0.00015313001605136436, 'epoch': 0.76}\n",
      "{'loss': 1.3114, 'learning_rate': 0.00015345104333868378, 'epoch': 0.77}\n",
      "{'loss': 1.2458, 'learning_rate': 0.00015377207062600322, 'epoch': 0.77}\n",
      "{'loss': 1.2125, 'learning_rate': 0.00015409309791332264, 'epoch': 0.77}\n",
      "{'loss': 1.2772, 'learning_rate': 0.00015441412520064208, 'epoch': 0.77}\n",
      "{'loss': 1.2633, 'learning_rate': 0.0001547351524879615, 'epoch': 0.77}\n",
      "{'loss': 1.2415, 'learning_rate': 0.0001550561797752809, 'epoch': 0.77}\n",
      "{'loss': 1.2337, 'learning_rate': 0.00015537720706260033, 'epoch': 0.78}\n",
      "{'loss': 1.2469, 'learning_rate': 0.00015569823434991975, 'epoch': 0.78}\n",
      "{'loss': 1.2382, 'learning_rate': 0.00015601926163723916, 'epoch': 0.78}\n",
      "{'loss': 1.229, 'learning_rate': 0.00015634028892455858, 'epoch': 0.78}\n",
      "{'loss': 1.2698, 'learning_rate': 0.000156661316211878, 'epoch': 0.78}\n",
      "{'loss': 1.2578, 'learning_rate': 0.00015698234349919744, 'epoch': 0.78}\n",
      "{'loss': 1.2999, 'learning_rate': 0.00015730337078651685, 'epoch': 0.79}\n",
      "{'loss': 1.3016, 'learning_rate': 0.0001576243980738363, 'epoch': 0.79}\n",
      "{'loss': 1.2778, 'learning_rate': 0.00015794542536115571, 'epoch': 0.79}\n",
      "{'loss': 1.2596, 'learning_rate': 0.00015826645264847513, 'epoch': 0.79}\n",
      "{'loss': 1.2463, 'learning_rate': 0.00015858747993579455, 'epoch': 0.79}\n",
      "{'loss': 1.1892, 'learning_rate': 0.00015890850722311396, 'epoch': 0.79}\n",
      "{'loss': 1.2119, 'learning_rate': 0.00015922953451043338, 'epoch': 0.8}\n",
      "{'loss': 1.1986, 'learning_rate': 0.0001595505617977528, 'epoch': 0.8}\n",
      "{'loss': 1.2727, 'learning_rate': 0.00015987158908507224, 'epoch': 0.8}\n",
      "{'loss': 1.2935, 'learning_rate': 0.00016019261637239166, 'epoch': 0.8}\n",
      "{'loss': 1.2515, 'learning_rate': 0.0001605136436597111, 'epoch': 0.8}\n",
      "{'loss': 1.2138, 'learning_rate': 0.00016083467094703052, 'epoch': 0.8}\n",
      "{'loss': 1.3058, 'learning_rate': 0.00016115569823434993, 'epoch': 0.8}\n",
      "{'loss': 1.2896, 'learning_rate': 0.00016147672552166935, 'epoch': 0.81}\n",
      "{'loss': 1.2446, 'learning_rate': 0.00016179775280898877, 'epoch': 0.81}\n",
      "{'loss': 1.275, 'learning_rate': 0.00016211878009630818, 'epoch': 0.81}\n",
      "{'loss': 1.2275, 'learning_rate': 0.0001624398073836276, 'epoch': 0.81}\n",
      "{'loss': 1.2976, 'learning_rate': 0.00016276083467094704, 'epoch': 0.81}\n",
      "{'loss': 1.26, 'learning_rate': 0.00016308186195826646, 'epoch': 0.81}\n",
      "{'loss': 1.2575, 'learning_rate': 0.00016340288924558588, 'epoch': 0.82}\n",
      "{'loss': 1.2765, 'learning_rate': 0.00016372391653290532, 'epoch': 0.82}\n",
      "{'loss': 1.1835, 'learning_rate': 0.00016404494382022474, 'epoch': 0.82}\n",
      "{'loss': 1.2015, 'learning_rate': 0.00016436597110754415, 'epoch': 0.82}\n",
      "{'loss': 1.2542, 'learning_rate': 0.00016468699839486357, 'epoch': 0.82}\n",
      "{'loss': 1.2624, 'learning_rate': 0.000165008025682183, 'epoch': 0.82}\n",
      "{'loss': 1.2206, 'learning_rate': 0.0001653290529695024, 'epoch': 0.83}\n",
      "{'loss': 1.2514, 'learning_rate': 0.00016565008025682182, 'epoch': 0.83}\n",
      "{'loss': 1.2143, 'learning_rate': 0.00016597110754414126, 'epoch': 0.83}\n",
      "{'loss': 1.1747, 'learning_rate': 0.00016629213483146068, 'epoch': 0.83}\n",
      "{'loss': 1.2548, 'learning_rate': 0.0001666131621187801, 'epoch': 0.83}\n",
      "{'loss': 1.3097, 'learning_rate': 0.00016693418940609954, 'epoch': 0.83}\n",
      "{'loss': 1.1877, 'learning_rate': 0.00016725521669341896, 'epoch': 0.84}\n",
      "{'loss': 1.2212, 'learning_rate': 0.00016757624398073837, 'epoch': 0.84}\n",
      "{'loss': 1.292, 'learning_rate': 0.0001678972712680578, 'epoch': 0.84}\n",
      "{'loss': 1.2254, 'learning_rate': 0.0001682182985553772, 'epoch': 0.84}\n",
      "{'loss': 1.2427, 'learning_rate': 0.00016853932584269662, 'epoch': 0.84}\n",
      "{'loss': 1.2377, 'learning_rate': 0.00016886035313001607, 'epoch': 0.84}\n",
      "{'loss': 1.1866, 'learning_rate': 0.00016918138041733548, 'epoch': 0.84}\n",
      "{'loss': 1.2098, 'learning_rate': 0.0001695024077046549, 'epoch': 0.85}\n",
      "{'loss': 1.1824, 'learning_rate': 0.00016982343499197432, 'epoch': 0.85}\n",
      "{'loss': 1.3487, 'learning_rate': 0.00017014446227929376, 'epoch': 0.85}\n",
      "{'loss': 1.2922, 'learning_rate': 0.00017046548956661318, 'epoch': 0.85}\n",
      "{'loss': 1.1512, 'learning_rate': 0.0001707865168539326, 'epoch': 0.85}\n",
      "{'loss': 1.2128, 'learning_rate': 0.000171107544141252, 'epoch': 0.85}\n",
      "{'loss': 1.2685, 'learning_rate': 0.00017142857142857143, 'epoch': 0.86}\n",
      "{'loss': 1.2255, 'learning_rate': 0.00017174959871589087, 'epoch': 0.86}\n",
      "{'loss': 1.2233, 'learning_rate': 0.00017207062600321029, 'epoch': 0.86}\n",
      "{'loss': 1.2342, 'learning_rate': 0.0001723916532905297, 'epoch': 0.86}\n",
      "{'loss': 1.212, 'learning_rate': 0.00017271268057784912, 'epoch': 0.86}\n",
      "{'loss': 1.2515, 'learning_rate': 0.00017303370786516853, 'epoch': 0.86}\n",
      "{'loss': 1.2645, 'learning_rate': 0.00017335473515248798, 'epoch': 0.87}\n",
      "{'loss': 1.2182, 'learning_rate': 0.0001736757624398074, 'epoch': 0.87}\n",
      "{'loss': 1.2188, 'learning_rate': 0.0001739967897271268, 'epoch': 0.87}\n",
      "{'loss': 1.2906, 'learning_rate': 0.00017431781701444623, 'epoch': 0.87}\n",
      "{'loss': 1.1656, 'learning_rate': 0.00017463884430176567, 'epoch': 0.87}\n",
      "{'loss': 1.2321, 'learning_rate': 0.0001749598715890851, 'epoch': 0.87}\n",
      "{'loss': 1.2485, 'learning_rate': 0.0001752808988764045, 'epoch': 0.88}\n",
      "{'loss': 1.2259, 'learning_rate': 0.00017560192616372392, 'epoch': 0.88}\n",
      "{'loss': 1.2529, 'learning_rate': 0.00017592295345104334, 'epoch': 0.88}\n",
      "{'loss': 1.2488, 'learning_rate': 0.00017624398073836278, 'epoch': 0.88}\n",
      "{'loss': 1.199, 'learning_rate': 0.0001765650080256822, 'epoch': 0.88}\n",
      "{'loss': 1.2109, 'learning_rate': 0.00017688603531300161, 'epoch': 0.88}\n",
      "{'loss': 1.2204, 'learning_rate': 0.00017720706260032103, 'epoch': 0.88}\n",
      "{'loss': 1.2803, 'learning_rate': 0.00017752808988764045, 'epoch': 0.89}\n",
      "{'loss': 1.2367, 'learning_rate': 0.0001778491171749599, 'epoch': 0.89}\n",
      "{'loss': 1.2549, 'learning_rate': 0.0001781701444622793, 'epoch': 0.89}\n",
      "{'loss': 1.2613, 'learning_rate': 0.00017849117174959872, 'epoch': 0.89}\n",
      "{'loss': 1.2729, 'learning_rate': 0.00017881219903691814, 'epoch': 0.89}\n",
      "{'loss': 1.213, 'learning_rate': 0.00017913322632423756, 'epoch': 0.89}\n",
      "{'loss': 1.1849, 'learning_rate': 0.000179454253611557, 'epoch': 0.9}\n",
      "{'loss': 1.2176, 'learning_rate': 0.00017977528089887642, 'epoch': 0.9}\n",
      "{'loss': 1.2358, 'learning_rate': 0.00018009630818619583, 'epoch': 0.9}\n",
      "{'loss': 1.2238, 'learning_rate': 0.00018041733547351525, 'epoch': 0.9}\n",
      "{'loss': 1.2171, 'learning_rate': 0.0001807383627608347, 'epoch': 0.9}\n",
      "{'loss': 1.2704, 'learning_rate': 0.0001810593900481541, 'epoch': 0.9}\n",
      "{'loss': 1.2451, 'learning_rate': 0.00018138041733547353, 'epoch': 0.91}\n",
      "{'loss': 1.2737, 'learning_rate': 0.00018170144462279294, 'epoch': 0.91}\n",
      "{'loss': 1.2591, 'learning_rate': 0.00018202247191011236, 'epoch': 0.91}\n",
      "{'loss': 1.2167, 'learning_rate': 0.00018234349919743178, 'epoch': 0.91}\n",
      "{'loss': 1.2464, 'learning_rate': 0.00018266452648475122, 'epoch': 0.91}\n",
      "{'loss': 1.2459, 'learning_rate': 0.00018298555377207064, 'epoch': 0.91}\n",
      "{'loss': 1.1985, 'learning_rate': 0.00018330658105939005, 'epoch': 0.92}\n",
      "{'loss': 1.253, 'learning_rate': 0.0001836276083467095, 'epoch': 0.92}\n",
      "{'loss': 1.1919, 'learning_rate': 0.0001839486356340289, 'epoch': 0.92}\n",
      "{'loss': 1.2318, 'learning_rate': 0.00018426966292134833, 'epoch': 0.92}\n",
      "{'loss': 1.2257, 'learning_rate': 0.00018459069020866775, 'epoch': 0.92}\n",
      "{'loss': 1.258, 'learning_rate': 0.00018491171749598716, 'epoch': 0.92}\n",
      "{'loss': 1.3041, 'learning_rate': 0.00018523274478330658, 'epoch': 0.92}\n",
      "{'loss': 1.2395, 'learning_rate': 0.000185553772070626, 'epoch': 0.93}\n",
      "{'loss': 1.2428, 'learning_rate': 0.00018587479935794544, 'epoch': 0.93}\n",
      "{'loss': 1.2312, 'learning_rate': 0.00018619582664526486, 'epoch': 0.93}\n",
      "{'loss': 1.251, 'learning_rate': 0.00018651685393258427, 'epoch': 0.93}\n",
      "{'loss': 1.2066, 'learning_rate': 0.00018683788121990372, 'epoch': 0.93}\n",
      "{'loss': 1.2237, 'learning_rate': 0.00018715890850722313, 'epoch': 0.93}\n",
      "{'loss': 1.2418, 'learning_rate': 0.00018747993579454255, 'epoch': 0.94}\n",
      "{'loss': 1.2004, 'learning_rate': 0.00018780096308186197, 'epoch': 0.94}\n",
      "{'loss': 1.2848, 'learning_rate': 0.00018812199036918138, 'epoch': 0.94}\n",
      "{'loss': 1.3215, 'learning_rate': 0.0001884430176565008, 'epoch': 0.94}\n",
      "{'loss': 1.2416, 'learning_rate': 0.00018876404494382024, 'epoch': 0.94}\n",
      "{'loss': 1.2593, 'learning_rate': 0.00018908507223113966, 'epoch': 0.94}\n",
      "{'loss': 1.2373, 'learning_rate': 0.00018940609951845908, 'epoch': 0.95}\n",
      "{'loss': 1.1547, 'learning_rate': 0.00018972712680577852, 'epoch': 0.95}\n",
      "{'loss': 1.1447, 'learning_rate': 0.00019004815409309794, 'epoch': 0.95}\n",
      "{'loss': 1.2241, 'learning_rate': 0.00019036918138041735, 'epoch': 0.95}\n",
      "{'loss': 1.2372, 'learning_rate': 0.00019069020866773677, 'epoch': 0.95}\n",
      "{'loss': 1.2566, 'learning_rate': 0.00019101123595505618, 'epoch': 0.95}\n",
      "{'loss': 1.2028, 'learning_rate': 0.0001913322632423756, 'epoch': 0.96}\n",
      "{'loss': 1.2734, 'learning_rate': 0.00019165329052969502, 'epoch': 0.96}\n",
      "{'loss': 1.2251, 'learning_rate': 0.00019197431781701446, 'epoch': 0.96}\n",
      "{'loss': 1.2436, 'learning_rate': 0.00019229534510433388, 'epoch': 0.96}\n",
      "{'loss': 1.1961, 'learning_rate': 0.00019261637239165332, 'epoch': 0.96}\n",
      "{'loss': 1.2438, 'learning_rate': 0.00019293739967897274, 'epoch': 0.96}\n",
      "{'loss': 1.2362, 'learning_rate': 0.00019325842696629215, 'epoch': 0.97}\n",
      "{'loss': 1.2095, 'learning_rate': 0.00019357945425361157, 'epoch': 0.97}\n",
      "{'loss': 1.3346, 'learning_rate': 0.000193900481540931, 'epoch': 0.97}\n",
      "{'loss': 1.1867, 'learning_rate': 0.0001942215088282504, 'epoch': 0.97}\n",
      "{'loss': 1.2182, 'learning_rate': 0.00019454253611556982, 'epoch': 0.97}\n",
      "{'loss': 1.2423, 'learning_rate': 0.00019486356340288924, 'epoch': 0.97}\n",
      "{'loss': 1.1791, 'learning_rate': 0.00019518459069020868, 'epoch': 0.97}\n",
      "{'loss': 1.3213, 'learning_rate': 0.0001955056179775281, 'epoch': 0.98}\n",
      "{'loss': 1.2786, 'learning_rate': 0.00019582664526484754, 'epoch': 0.98}\n",
      "{'loss': 1.1608, 'learning_rate': 0.00019614767255216696, 'epoch': 0.98}\n",
      "{'loss': 1.1966, 'learning_rate': 0.00019646869983948637, 'epoch': 0.98}\n",
      "{'loss': 1.2245, 'learning_rate': 0.0001967897271268058, 'epoch': 0.98}\n",
      "{'loss': 1.2742, 'learning_rate': 0.0001971107544141252, 'epoch': 0.98}\n",
      "{'loss': 1.2623, 'learning_rate': 0.00019743178170144462, 'epoch': 0.99}\n",
      "{'loss': 1.2176, 'learning_rate': 0.00019775280898876404, 'epoch': 0.99}\n",
      "{'loss': 1.2209, 'learning_rate': 0.00019807383627608346, 'epoch': 0.99}\n",
      "{'loss': 1.2425, 'learning_rate': 0.0001983948635634029, 'epoch': 0.99}\n",
      "{'loss': 1.1896, 'learning_rate': 0.00019871589085072234, 'epoch': 0.99}\n",
      "{'loss': 1.1751, 'learning_rate': 0.00019903691813804176, 'epoch': 0.99}\n",
      "{'loss': 1.2093, 'learning_rate': 0.00019935794542536118, 'epoch': 1.0}\n",
      "{'loss': 1.2425, 'learning_rate': 0.0001996789727126806, 'epoch': 1.0}\n",
      "{'loss': 1.2377, 'learning_rate': 0.0002, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "462a4c04d5054805ab9b2bb51bf17273",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/201 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.2453058958053589, 'eval_runtime': 143.0478, 'eval_samples_per_second': 1.405, 'eval_steps_per_second': 1.405, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bf32fd202474eef91e75013546b9947",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\output\\falcon-1b\\checkpoint-623)... Done. 0.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2773, 'learning_rate': 0.00019999998430329032, 'epoch': 1.0}\n",
      "{'loss': 1.2079, 'learning_rate': 0.00019999993721316617, 'epoch': 1.0}\n",
      "{'loss': 1.2171, 'learning_rate': 0.00019999985872964232, 'epoch': 1.0}\n",
      "{'loss': 1.2262, 'learning_rate': 0.00019999974885274348, 'epoch': 1.01}\n",
      "{'loss': 1.1884, 'learning_rate': 0.00019999960758250408, 'epoch': 1.01}\n",
      "{'loss': 1.2091, 'learning_rate': 0.00019999943491896848, 'epoch': 1.01}\n",
      "{'loss': 1.2155, 'learning_rate': 0.0001999992308621909, 'epoch': 1.01}\n",
      "{'loss': 1.2788, 'learning_rate': 0.0001999989954122354, 'epoch': 1.01}\n",
      "{'loss': 1.2888, 'learning_rate': 0.0001999987285691759, 'epoch': 1.01}\n",
      "{'loss': 1.2152, 'learning_rate': 0.00019999843033309612, 'epoch': 1.01}\n",
      "{'loss': 1.2881, 'learning_rate': 0.00019999810070408974, 'epoch': 1.02}\n",
      "{'loss': 1.2868, 'learning_rate': 0.00019999773968226023, 'epoch': 1.02}\n",
      "{'loss': 1.2385, 'learning_rate': 0.00019999734726772092, 'epoch': 1.02}\n",
      "{'loss': 1.2182, 'learning_rate': 0.00019999692346059502, 'epoch': 1.02}\n",
      "{'loss': 1.2, 'learning_rate': 0.00019999646826101558, 'epoch': 1.02}\n",
      "{'loss': 1.1652, 'learning_rate': 0.00019999598166912545, 'epoch': 1.02}\n",
      "{'loss': 1.2411, 'learning_rate': 0.00019999546368507746, 'epoch': 1.03}\n",
      "{'loss': 1.2486, 'learning_rate': 0.0001999949143090342, 'epoch': 1.03}\n",
      "{'loss': 1.2195, 'learning_rate': 0.00019999433354116807, 'epoch': 1.03}\n",
      "{'loss': 1.283, 'learning_rate': 0.00019999372138166147, 'epoch': 1.03}\n",
      "{'loss': 1.2708, 'learning_rate': 0.0001999930778307066, 'epoch': 1.03}\n",
      "{'loss': 1.1985, 'learning_rate': 0.0001999924028885054, 'epoch': 1.03}\n",
      "{'loss': 1.2463, 'learning_rate': 0.00019999169655526982, 'epoch': 1.04}\n",
      "{'loss': 1.2522, 'learning_rate': 0.0001999909588312216, 'epoch': 1.04}\n",
      "{'loss': 1.1797, 'learning_rate': 0.00019999018971659233, 'epoch': 1.04}\n",
      "{'loss': 1.231, 'learning_rate': 0.00019998938921162343, 'epoch': 1.04}\n",
      "{'loss': 1.097, 'learning_rate': 0.00019998855731656628, 'epoch': 1.04}\n",
      "{'loss': 1.1623, 'learning_rate': 0.00019998769403168197, 'epoch': 1.04}\n",
      "{'loss': 1.1895, 'learning_rate': 0.0001999867993572415, 'epoch': 1.05}\n",
      "{'loss': 1.2223, 'learning_rate': 0.00019998587329352586, 'epoch': 1.05}\n",
      "{'loss': 1.2366, 'learning_rate': 0.00019998491584082562, 'epoch': 1.05}\n",
      "{'loss': 1.3251, 'learning_rate': 0.00019998392699944147, 'epoch': 1.05}\n",
      "{'loss': 1.2455, 'learning_rate': 0.00019998290676968378, 'epoch': 1.05}\n",
      "{'loss': 1.2216, 'learning_rate': 0.00019998185515187285, 'epoch': 1.05}\n",
      "{'loss': 1.1988, 'learning_rate': 0.00019998077214633883, 'epoch': 1.05}\n",
      "{'loss': 1.2098, 'learning_rate': 0.0001999796577534217, 'epoch': 1.06}\n",
      "{'loss': 1.2473, 'learning_rate': 0.0001999785119734713, 'epoch': 1.06}\n",
      "{'loss': 1.2645, 'learning_rate': 0.00019997733480684737, 'epoch': 1.06}\n",
      "{'loss': 1.1647, 'learning_rate': 0.00019997612625391944, 'epoch': 1.06}\n",
      "{'loss': 1.1907, 'learning_rate': 0.00019997488631506686, 'epoch': 1.06}\n",
      "{'loss': 1.3273, 'learning_rate': 0.00019997361499067898, 'epoch': 1.06}\n",
      "{'loss': 1.2104, 'learning_rate': 0.00019997231228115485, 'epoch': 1.07}\n",
      "{'loss': 1.2081, 'learning_rate': 0.00019997097818690346, 'epoch': 1.07}\n",
      "{'loss': 1.1647, 'learning_rate': 0.00019996961270834365, 'epoch': 1.07}\n",
      "{'loss': 1.2559, 'learning_rate': 0.00019996821584590405, 'epoch': 1.07}\n",
      "{'loss': 1.1707, 'learning_rate': 0.00019996678760002317, 'epoch': 1.07}\n",
      "{'loss': 1.2159, 'learning_rate': 0.00019996532797114945, 'epoch': 1.07}\n",
      "{'loss': 1.1974, 'learning_rate': 0.00019996383695974107, 'epoch': 1.08}\n",
      "{'loss': 1.2461, 'learning_rate': 0.00019996231456626608, 'epoch': 1.08}\n",
      "{'loss': 1.2137, 'learning_rate': 0.0001999607607912025, 'epoch': 1.08}\n",
      "{'loss': 1.2109, 'learning_rate': 0.00019995917563503802, 'epoch': 1.08}\n",
      "{'loss': 1.1912, 'learning_rate': 0.00019995755909827038, 'epoch': 1.08}\n",
      "{'loss': 1.2805, 'learning_rate': 0.00019995591118140697, 'epoch': 1.08}\n",
      "{'loss': 1.2662, 'learning_rate': 0.00019995423188496516, 'epoch': 1.09}\n",
      "{'loss': 1.2705, 'learning_rate': 0.00019995252120947213, 'epoch': 1.09}\n",
      "{'loss': 1.2283, 'learning_rate': 0.00019995077915546493, 'epoch': 1.09}\n",
      "{'loss': 1.2533, 'learning_rate': 0.0001999490057234905, 'epoch': 1.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Leandro\\AppData\\Local\\Temp\\ipykernel_25032\\3042643859.py\", line 2, in <module>\n",
      "    trainer.train()\n",
      "  File \"c:\\Users\\Leandro\\source\\repos\\PyTorchTest\\PyTorchTest\\TorchEnv\\lib\\site-packages\\trl\\trainer\\sft_trainer.py\", line 315, in train\n",
      "    output = super().train(*args, **kwargs)\n",
      "  File \"c:\\Users\\Leandro\\source\\repos\\PyTorchTest\\PyTorchTest\\TorchEnv\\lib\\site-packages\\transformers\\trainer.py\", line 1537, in train\n",
      "    return inner_training_loop(\n",
      "  File \"c:\\Users\\Leandro\\source\\repos\\PyTorchTest\\PyTorchTest\\TorchEnv\\lib\\site-packages\\transformers\\trainer.py\", line 1854, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs)\n",
      "  File \"c:\\Users\\Leandro\\source\\repos\\PyTorchTest\\PyTorchTest\\TorchEnv\\lib\\site-packages\\transformers\\trainer.py\", line 2744, in training_step\n",
      "    self.accelerator.backward(loss)\n",
      "  File \"c:\\Users\\Leandro\\source\\repos\\PyTorchTest\\PyTorchTest\\TorchEnv\\lib\\site-packages\\accelerate\\accelerator.py\", line 1903, in backward\n",
      "    self.scaler.scale(loss).backward(**kwargs)\n",
      "  File \"c:\\Users\\Leandro\\source\\repos\\PyTorchTest\\PyTorchTest\\TorchEnv\\lib\\site-packages\\torch\\_tensor.py\", line 492, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"c:\\Users\\Leandro\\source\\repos\\PyTorchTest\\PyTorchTest\\TorchEnv\\lib\\site-packages\\torch\\autograd\\__init__.py\", line 251, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "022ff5f3a147459a8de078bf6df2b740",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='147.461 MB of 147.461 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁▁</td></tr><tr><td>eval/runtime</td><td>▁▁</td></tr><tr><td>eval/samples_per_second</td><td>▁▁</td></tr><tr><td>eval/steps_per_second</td><td>▁▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/learning_rate</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇██████</td></tr><tr><td>train/loss</td><td>█▇▇▆▅▅▄▃▃▃▃▂▃▂▂▂▂▂▂▂▂▂▂▂▃▂▁▂▂▂▂▂▂▂▁▁▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.24531</td></tr><tr><td>eval/runtime</td><td>143.0478</td></tr><tr><td>eval/samples_per_second</td><td>1.405</td></tr><tr><td>eval/steps_per_second</td><td>1.405</td></tr><tr><td>train/epoch</td><td>1.09</td></tr><tr><td>train/global_step</td><td>680</td></tr><tr><td>train/learning_rate</td><td>0.0002</td></tr><tr><td>train/loss</td><td>1.2533</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">falcon-1b</strong> at: <a href='https://wandb.ai/leandro-bello/LLM_alpaca/runs/n8itsr3a' target=\"_blank\">https://wandb.ai/leandro-bello/LLM_alpaca/runs/n8itsr3a</a><br/>Synced 5 W&B file(s), 1 media file(s), 14 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240317_142310-n8itsr3a\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m wandb\u001b[38;5;241m.\u001b[39minit(project\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLLM_alpaca\u001b[39m\u001b[38;5;124m'\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfalcon-1b\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Leandro\\source\\repos\\PyTorchTest\\PyTorchTest\\TorchEnv\\lib\\site-packages\\trl\\trainer\\sft_trainer.py:315\u001b[0m, in \u001b[0;36mSFTTrainer.train\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n\u001b[0;32m    313\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trl_activate_neftune(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[1;32m--> 315\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    317\u001b[0m \u001b[38;5;66;03m# After training we make sure to retrieve back the original forward pass method\u001b[39;00m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;66;03m# for the embedding layer by removing the forward post hook.\u001b[39;00m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n",
      "File \u001b[1;32mc:\\Users\\Leandro\\source\\repos\\PyTorchTest\\PyTorchTest\\TorchEnv\\lib\\site-packages\\transformers\\trainer.py:1537\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1535\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1537\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1538\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1539\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1540\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1542\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Leandro\\source\\repos\\PyTorchTest\\PyTorchTest\\TorchEnv\\lib\\site-packages\\transformers\\trainer.py:1854\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1851\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m   1853\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m-> 1854\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1856\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1857\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1858\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   1859\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   1860\u001b[0m ):\n\u001b[0;32m   1861\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1862\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\Leandro\\source\\repos\\PyTorchTest\\PyTorchTest\\TorchEnv\\lib\\site-packages\\transformers\\trainer.py:2744\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2742\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m   2743\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2744\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2746\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[1;32mc:\\Users\\Leandro\\source\\repos\\PyTorchTest\\PyTorchTest\\TorchEnv\\lib\\site-packages\\accelerate\\accelerator.py:1903\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   1901\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   1902\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1903\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1904\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1905\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Leandro\\source\\repos\\PyTorchTest\\PyTorchTest\\TorchEnv\\lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Leandro\\source\\repos\\PyTorchTest\\PyTorchTest\\TorchEnv\\lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with wandb.init(project='LLM_alpaca', name='falcon-1b'):\n",
    "    trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TorchEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
